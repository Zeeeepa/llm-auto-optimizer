//! Markdown Report Generation
//!
//! This module provides functionality for generating markdown reports
//! from benchmark results. It creates human-readable summaries suitable
//! for documentation and CI/CD output.

use crate::BenchmarkResult;
use chrono::Utc;

/// Generate a markdown report from benchmark results.
///
/// # Arguments
///
/// * `results` - Slice of benchmark results to include in the report
///
/// # Returns
///
/// A formatted markdown string containing the benchmark summary.
pub fn generate_report(results: &[BenchmarkResult]) -> String {
    let mut report = String::new();

    // Header
    report.push_str("# LLM Auto-Optimizer Benchmark Results\n\n");
    report.push_str(&format!(
        "**Generated:** {}\n\n",
        Utc::now().format("%Y-%m-%d %H:%M:%S UTC")
    ));

    // Summary statistics
    let total = results.len();
    let successful = results.iter().filter(|r| r.is_success()).count();
    let failed = total - successful;
    let total_time: u64 = results
        .iter()
        .filter_map(|r| r.execution_time_ms())
        .sum();

    report.push_str("## Summary\n\n");
    report.push_str(&format!("| Metric | Value |\n"));
    report.push_str(&format!("|--------|-------|\n"));
    report.push_str(&format!("| Total Benchmarks | {} |\n", total));
    report.push_str(&format!("| Successful | {} |\n", successful));
    report.push_str(&format!("| Failed | {} |\n", failed));
    report.push_str(&format!("| Total Execution Time | {}ms |\n", total_time));
    report.push_str("\n");

    // Results table
    report.push_str("## Benchmark Results\n\n");
    report.push_str("| Target ID | Status | Execution Time | Timestamp |\n");
    report.push_str("|-----------|--------|----------------|------------|\n");

    for result in results {
        let status = if result.is_success() { "✓ Pass" } else { "✗ Fail" };
        let exec_time = result
            .execution_time_ms()
            .map(|t| format!("{}ms", t))
            .unwrap_or_else(|| "N/A".to_string());
        let timestamp = result.timestamp.format("%H:%M:%S");

        report.push_str(&format!(
            "| {} | {} | {} | {} |\n",
            result.target_id, status, exec_time, timestamp
        ));
    }
    report.push_str("\n");

    // Detailed metrics for each benchmark
    report.push_str("## Detailed Metrics\n\n");

    for result in results {
        report.push_str(&format!("### {}\n\n", result.target_id));

        if let Some(obj) = result.metrics.as_object() {
            report.push_str("```json\n");
            report.push_str(&serde_json::to_string_pretty(&result.metrics).unwrap_or_default());
            report.push_str("\n```\n\n");

            // Extract key metrics into a table
            let key_metrics: Vec<_> = obj
                .iter()
                .filter(|(k, _)| !k.starts_with("_") && *k != "execution_time_ms")
                .collect();

            if !key_metrics.is_empty() {
                report.push_str("| Metric | Value |\n");
                report.push_str("|--------|-------|\n");

                for (key, value) in key_metrics {
                    let value_str = match value {
                        serde_json::Value::Number(n) => {
                            if let Some(f) = n.as_f64() {
                                if f.fract() == 0.0 {
                                    format!("{}", f as i64)
                                } else {
                                    format!("{:.4}", f)
                                }
                            } else {
                                n.to_string()
                            }
                        }
                        serde_json::Value::Bool(b) => b.to_string(),
                        serde_json::Value::String(s) => s.clone(),
                        _ => value.to_string(),
                    };
                    report.push_str(&format!("| {} | {} |\n", key, value_str));
                }
                report.push_str("\n");
            }
        }
    }

    // Footer
    report.push_str("---\n\n");
    report.push_str("*Generated by LLM Auto-Optimizer Benchmark Suite*\n");

    report
}

/// Generate a compact summary suitable for CI output.
///
/// # Arguments
///
/// * `results` - Slice of benchmark results
///
/// # Returns
///
/// A single-line summary string.
pub fn generate_summary_line(results: &[BenchmarkResult]) -> String {
    let total = results.len();
    let successful = results.iter().filter(|r| r.is_success()).count();
    let total_time: u64 = results
        .iter()
        .filter_map(|r| r.execution_time_ms())
        .sum();

    format!(
        "Benchmarks: {}/{} passed in {}ms",
        successful, total, total_time
    )
}

/// Generate a badge-compatible status.
///
/// # Arguments
///
/// * `results` - Slice of benchmark results
///
/// # Returns
///
/// A tuple of (label, message, color) suitable for shields.io badges.
pub fn generate_badge_status(results: &[BenchmarkResult]) -> (&'static str, String, &'static str) {
    let total = results.len();
    let successful = results.iter().filter(|r| r.is_success()).count();

    let color = if successful == total {
        "brightgreen"
    } else if successful > total / 2 {
        "yellow"
    } else {
        "red"
    };

    ("benchmarks", format!("{}/{} passing", successful, total), color)
}

#[cfg(test)]
mod tests {
    use super::*;
    use serde_json::json;

    fn sample_results() -> Vec<BenchmarkResult> {
        vec![
            BenchmarkResult::new(
                "pareto-optimization",
                json!({
                    "success": true,
                    "execution_time_ms": 50,
                    "candidates_evaluated": 10,
                    "pareto_optimal_count": 3
                }),
            ),
            BenchmarkResult::new(
                "model-selection",
                json!({
                    "success": true,
                    "execution_time_ms": 30,
                    "models_compared": 5
                }),
            ),
        ]
    }

    #[test]
    fn test_generate_report() {
        let results = sample_results();
        let report = generate_report(&results);

        assert!(report.contains("# LLM Auto-Optimizer Benchmark Results"));
        assert!(report.contains("pareto-optimization"));
        assert!(report.contains("model-selection"));
        assert!(report.contains("Total Benchmarks"));
        assert!(report.contains("| 2 |"));
    }

    #[test]
    fn test_generate_summary_line() {
        let results = sample_results();
        let summary = generate_summary_line(&results);

        assert!(summary.contains("2/2 passed"));
        assert!(summary.contains("80ms")); // 50 + 30
    }

    #[test]
    fn test_generate_badge_status() {
        let results = sample_results();
        let (label, message, color) = generate_badge_status(&results);

        assert_eq!(label, "benchmarks");
        assert!(message.contains("2/2"));
        assert_eq!(color, "brightgreen");
    }

    #[test]
    fn test_badge_status_with_failures() {
        let results = vec![
            BenchmarkResult::new("test1", json!({ "success": true })),
            BenchmarkResult::new("test2", json!({ "success": false })),
        ];

        let (_, _, color) = generate_badge_status(&results);
        assert_eq!(color, "yellow"); // 1/2 passed
    }
}
